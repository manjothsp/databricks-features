Data Lake -> storage repository that holds vast amount of data in native format in form of files.

benefits:
============
1) cost effective
2) scalability
Can hold any kind of data. structured/semi/unstructured.

challenges of data lake: do not provide ACID guarantees, leading to data quality issues. NO versioning and no DML operations.
======================

1) job is failing while appending the new data: out of 5 new part files, only 3 were written.
2) job is failing while overwriting the data: while overwriting original files were deleted,
3) simultaneous read and write: while appending new files, some user try to read the data leading to inconsistency.

========================
ACID :
=======
automicity
consistency
isolation
durability


=======================|
DELTA LAKE:    open source storage layer. Similar to parquet but with addition of transaction log.
Parquet + transaction logs(_delta_log)

for each write operation -> parquet files are written first and then delta_log(in json format)
for each read operation -> delta_log is read first and then corresponding parquet files are read.

All 3 scenerios above are solved by delta lake.



Insert/Update/Delete Operations:
=================================

1) Insert -> when new records are inserted , new parquet file is added and a new transaction file in _delta_log
2) Delete -> when a record is deleted, a deletion vector is added(tombstone marker), which indicates while reading data do not include the deleted record.
No new files are added as part of delete optimization
3) Update -> updated records are kept in new parquet file and deletion vector is added for the old records. So while reading, will read new file + old file + deletion vector.

** When we create a managed table even after giving location at catalog level, we cannot see the underlying files from databricks using 
dbutils.fs.ls().

