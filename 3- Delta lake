Data Lake -> storage repository that holds vast amount of data in native format in form of files.

benefits:
============
1) cost effective
2) scalability
Can hold any kind of data. structured/semi/unstructured.

challenges of data lake: do not provide ACID guarantees, leading to data quality issues. NO versioning and no DML operations.
======================

1) job is failing while appending the new data: out of 5 new part files, only 3 were written.
2) job is failing while overwriting the data: while overwriting original files were deleted,
3) simultaneous read and write: while appending new files, some user try to read the data leading to inconsistency.

========================
ACID :
=======
automicity
consistency
isolation
durability


=======================|
DELTA LAKE:    open source storage layer. Similar to parquet but with addition of transaction log.
Parquet + transaction logs(_delta_log)

for each write operation -> parquet files are written first and then delta_log(in json format)
for each read operation -> delta_log is read first and then corresponding parquet files are read.

All 3 scenerios above are solved by delta lake.
** When we create a managed table even after giving location at catalog level, we cannot see the underlying files from databricks using 
dbutils.fs.ls().


Insert/Update/Delete Operations:
=================================

1) Insert -> when new records are inserted , new parquet file is added and a new transaction file in _delta_log
2) Delete -> when a record is deleted, a deletion vector is added(tombstone marker), which indicates while reading data do not include the deleted record.
No new files are added as part of delete optimization
3) Update -> updated records are kept in new parquet file and deletion vector is added for the old records. So while reading, will read new file + old file + deletion vector.


TimeTravel , Vacuum and _delta_log commits:
==========================================

Delta allows time travel using versions which are checked via describe history command.
Time travel can be done based on version or asOfTimestamp eg: table@v2

Vacuum -> allows to cleanup parquet files that are no longer referenced in latest version and are older than 7 days.
        Vacuum can impact the time travel activities.
        Vacuum table DRY run;
%sql
spark.conf.set('spark.databricks.delta.retentionDurationCheck.enabled',False)
%sql
vacuum delta.`/Volumes/my_catalog/test/test_volume/test_marks/` retain 0 hours dry run


Commits -> If job is inserting multiple records on each run, it will keep on accumulating json files in _delta_log folder.
          Now while reading latest version , it has to run through all json files.
          Delta optimized this by 
          1) combining multiple json files to 1 after regular interval , creating a compacted.json file.
        dbfs:/Volumes/my_catalog/test/test_volume/test_marks/_delta_log/00000000000000000101.00000000000000000150.compacted.json

          2) After each 36 files, delta creates a checkpoint.parquet file by combining all json files.
            Delta maintains latest checkpoint number in a new file in _delta_log folder.
      dbfs:/Volumes/my_catalog/test/test_volume/test_marks/_delta_log/00000000000000000200.checkpoint.parquet




